{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/romellfudi/medium/blob/main/Bard_in_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#@title Import a Google ID Session (Must run it)\n",
        "!pip install bardapi gradio speechbrain git+https://github.com/openai/whisper.git -q\n",
        "#@markdown     Warning Do not expose the __Secure-1PSID\n",
        "#@markdown 1. Visit https://bard.google.com/\n",
        "#@markdown 2.  F12 for console\n",
        "#@markdown 3. Session: Application → Cookies → Copy the value of __Secure-1PSID cookie.\n",
        "base_system = \"Im going to ask questions as normal, however your response must be concisse, do not give me extra information unless I tell you. Do not try to translate.\"\n",
        "token='' #@param  {type: \"string\"}"
      ],
      "metadata": {
        "id": "-XDxj6OddJPa",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "whisper.available_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfoCvGLffii6",
        "outputId": "77b20f80-5c6d-42a1-b9e3-6e9498f739f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tiny.en',\n",
              " 'tiny',\n",
              " 'base.en',\n",
              " 'base',\n",
              " 'small.en',\n",
              " 'small',\n",
              " 'medium.en',\n",
              " 'medium',\n",
              " 'large-v1',\n",
              " 'large-v2',\n",
              " 'large']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import whisper\n",
        "import random\n",
        "from speechbrain.pretrained import EncoderDecoderASR\n",
        "import gradio as gr\n",
        "import requests\n",
        "from bardapi import Bard\n",
        "from IPython.display import Audio, display, Markdown\n",
        "\n",
        "def make_connection_to_bard(token):\n",
        "    session = requests.Session()\n",
        "    session.headers = {\n",
        "        \"Host\": \"bard.google.com\",\n",
        "        \"X-Same-Domain\": \"1\",\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\",\n",
        "        \"Content-Type\": \"application/x-www-form-urlencoded;charset=UTF-8\",\n",
        "        \"Origin\": \"https://bard.google.com\",\n",
        "        \"Referer\": \"https://bard.google.com/\",\n",
        "    }\n",
        "    session.cookies.set(\"__Secure-1PSID\", token)\n",
        "    return Bard(token=token, session=session)\n",
        "\n",
        "def clean_code_blocks(full_chat):\n",
        "    return re.sub(r\"```[\\s\\S]*?```\", \"\", full_chat)\n",
        "\n",
        "def transcribe(file_wav, transcriptor=\"speechbrain\"):\n",
        "    global asr_model\n",
        "    global whiser_base_model\n",
        "    global whiser_tiny_model\n",
        "    global whiser_large_model\n",
        "    transcription = \".\"\n",
        "    if transcriptor == \"speechbrain\":\n",
        "        transcription = asr_model.transcribe_file(file_wav)\n",
        "    elif transcriptor == \"base_whisper\":\n",
        "        transcription = whiser_base_model.transcribe(file_wav)[\"text\"]\n",
        "    elif transcriptor == \"tiny.en_whisper\":\n",
        "        transcription = whiser_tiny_model.transcribe(file_wav)[\"text\"]\n",
        "    elif transcriptor == \"large_whisper\":\n",
        "        transcription = whiser_large_model.transcribe(file_wav)[\"text\"]\n",
        "    return transcription\n",
        "\n",
        "\n",
        "chat_history = []\n",
        "global asr_model\n",
        "global whiser_tiny_model\n",
        "global whiser_base_model\n",
        "global whiser_large_model\n",
        "# asr_model = EncoderDecoderASR.from_hparams(source=\"speechbrain/asr-crdnn-rnnlm-librispeech\", savedir=\"pretrained_models/asr-crdnn-rnnlm-librispeech\")\n",
        "whiser_base_model = whisper.load_model(\"base\")\n",
        "# whiser_tiny_model = whisper.load_model(\"tiny.en\")\n",
        "# whiser_large_model = whisper.load_model(\"large\")\n",
        "bard = make_connection_to_bard(token)\n",
        "bard.get_answer(base_system)\n",
        "\n",
        "if not os.path.exists(\"bard.ogg\"):\n",
        "    with open(\"bard.ogg\", \"wb\") as f:\n",
        "        f.write(bytes(bard.speech(\"Hello, I am Google Bard AI. Please, let's chat.\")['audio']))\n",
        "\n",
        "if not os.path.exists(\"bard.ogg\"):\n",
        "    with open(\"empty.ogg\", \"wb\") as f:\n",
        "        f.write(bytes(bard.speech(\" \")['audio']))\n",
        "\n",
        "# Initialize Gradio interface with custom layout\n",
        "with gr.Blocks() as app:\n",
        "    gr.Markdown(\"# Welcome to the Bard Chatbot!\")\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "          gr.Markdown(\"## Record your message and chat with Bard.\")\n",
        "          audio_input = gr.Audio(label=\"Input Audio Channel\", source=\"microphone\", type=\"filepath\",)\n",
        "          transcriptor_options = [\n",
        "              # \"tiny.en_whisper\",\n",
        "              \"base_whisper\",\n",
        "              # \"large_whisper\",\n",
        "              # \"speechbrain\",\n",
        "          ]\n",
        "          transcriptor = gr.Radio(transcriptor_options, value=\"base_whisper\", label=\"Transcriptor\")\n",
        "\n",
        "      with gr.Column():\n",
        "          gr.Markdown(\"## Reproduce the Bot Answer\")\n",
        "          response_text = gr.Markdown()\n",
        "          audio_output = gr.Audio(\"bard.ogg\", label=\"Audio Output\", autoplay=True)\n",
        "          elapsed_time = gr.Text(label=\"Elapsed Time\")\n",
        "\n",
        "    with gr.Column():\n",
        "        chatbot = gr.Chatbot()\n",
        "\n",
        "\n",
        "    @gr.on(inputs=[audio_input, transcriptor], outputs=[audio_output, elapsed_time, chatbot])\n",
        "    def chat_with_bard(file_wav, transcriptor):\n",
        "        import time\n",
        "        start_time = time.time()\n",
        "        if file_wav is None:\n",
        "          # Not modify the output just skip\n",
        "          return \"empty.ogg\", f\"{0} seg.\", chat_history\n",
        "        user_input = transcribe(file_wav, transcriptor)\n",
        "        raw = bard.get_answer(user_input)\n",
        "        bot_message = random.choice(raw['choices'])['content'][0]\n",
        "        if raw['images']:\n",
        "            image = random.choice(raw['images'])\n",
        "            image_markdown = f\"![]({image})\\n\\n\"\n",
        "        else:\n",
        "            image_markdown = \"\"\n",
        "        full_response = image_markdown+bot_message\n",
        "        # print(full_response)\n",
        "        chat_history.append((user_input, full_response))\n",
        "\n",
        "        with open(\"bard.ogg\", \"wb\") as f:\n",
        "            f.write(bytes(bard.speech(clean_code_blocks(bot_message))['audio']))\n",
        "        return \"bard.ogg\", f\"{round(time.time() - start_time)} seg.\", chat_history\n",
        "\n",
        "\n",
        "# Launch the Gradio interface\n",
        "app.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "4l6MgiX6gmR8",
        "outputId": "95782194-fad5-41be-b661-cc6e0cf2915e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://fa2bf15aae599ec90a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://fa2bf15aae599ec90a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GKD__WYg41lF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}